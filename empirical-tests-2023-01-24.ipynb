{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b3b35533",
   "metadata": {},
   "source": [
    "## Agenda\n",
    "\n",
    "- Run a few examples with GPT-Neo and GPT-J"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e9d8c4d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# User definitions\n",
    "MODEL_NAME = \"EleutherAI/gpt-neo-125M\"\n",
    "MAX_TOKENS = 10\n",
    "DEVICE = \"gpu\"\n",
    "\n",
    "SEED = 9823\n",
    "NUM_SAMPLES = 500"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "63f733e7",
   "metadata": {},
   "source": [
    "## Naive Sampling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "5d090d8a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using pad_token, but it is not set yet.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Importing classes for model EleutherAI/gpt-neo-125M\n",
      " -> <class 'transformers.models.gpt2.tokenization_gpt2.GPT2Tokenizer'>\n",
      " -> <class 'transformers.models.gpt_neo.modeling_gpt_neo.GPTNeoForCausalLM'>\n",
      "Vocabulary size: 50257\n"
     ]
    }
   ],
   "source": [
    "from naive_sampler import NaiveSampler\n",
    "\n",
    "nsampler = NaiveSampler(MODEL_NAME, device=\"cuda\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f4af92d7",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 10/10 [00:00<00:00, 16.28it/s]\n"
     ]
    }
   ],
   "source": [
    "naive_probs, naive_samples = nsampler.estimate(\n",
    "    input_str = \"I find \",\n",
    "    avoid_terms = \"I I am you he she her him they them my yours\",\n",
    "    num_sequences=NUM_SAMPLES,\n",
    "    max_num_tokens=MAX_TOKENS,\n",
    "    seed=SEED,\n",
    ")\n",
    "\n",
    "nsampler.reset_intermediate_results()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "38a35ebf",
   "metadata": {},
   "source": [
    "## Importance sampling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "11283cfe",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using pad_token, but it is not set yet.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Importing classes for model EleutherAI/gpt-neo-125M\n",
      " -> <class 'transformers.models.gpt2.tokenization_gpt2.GPT2Tokenizer'>\n",
      " -> <class 'transformers.models.gpt_neo.modeling_gpt_neo.GPTNeoForCausalLM'>\n",
      "Vocabulary size: 50257\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|                                                                                                                                                                                                                        | 0/10 [00:00<?, ?it/s]\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "Expected all tensors to be on the same device, but found at least two devices, cuda:0 and cpu!",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn [4], line 4\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mimportance_sampler\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m ImportanceSampler\n\u001b[1;32m      2\u001b[0m isampler \u001b[38;5;241m=\u001b[39m ImportanceSampler(MODEL_NAME, device\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcuda\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m----> 4\u001b[0m importance_probs, importance_samples \u001b[38;5;241m=\u001b[39m \u001b[43misampler\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mestimate\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m      5\u001b[0m \u001b[43m    \u001b[49m\u001b[43minput_str\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mI find \u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m      6\u001b[0m \u001b[43m    \u001b[49m\u001b[43mavoid_terms\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mI I am you he she her him they them my yours\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m      7\u001b[0m \u001b[43m    \u001b[49m\u001b[43mnum_sequences\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mNUM_SAMPLES\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m      8\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmax_num_tokens\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mMAX_TOKENS\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m      9\u001b[0m \u001b[43m    \u001b[49m\u001b[43mseed\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mSEED\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     10\u001b[0m \u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Projects/PhD/constrained-decoding/base.py:117\u001b[0m, in \u001b[0;36mBaseSampler.estimate\u001b[0;34m(self, input_str, avoid_terms, num_sequences, max_num_tokens, seed, add_special_tokens)\u001b[0m\n\u001b[1;32m    114\u001b[0m \u001b[38;5;66;03m# Avoid duplicate ids (FIXME: May not make sense, when we add support for phrases)\u001b[39;00m\n\u001b[1;32m    115\u001b[0m avoid_terms_ids \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mtensor(avoid_terms_ids)\u001b[38;5;241m.\u001b[39msqueeze()\u001b[38;5;241m.\u001b[39munique()\n\u001b[0;32m--> 117\u001b[0m results \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_sample\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    118\u001b[0m \u001b[43m    \u001b[49m\u001b[43mavoid_terms_ids\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mavoid_terms_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    119\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmax_num_tokens\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmax_num_tokens\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    120\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43msampling_specific_kwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    121\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    123\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m results\n",
      "File \u001b[0;32m~/Projects/PhD/constrained-decoding/importance_sampler.py:123\u001b[0m, in \u001b[0;36mImportanceSampler._sample\u001b[0;34m(self, input_ids, avoid_terms_ids, max_num_tokens, model_kwargs)\u001b[0m\n\u001b[1;32m    116\u001b[0m     unfinished_sequences \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mlogical_and(\n\u001b[1;32m    117\u001b[0m         unfinished_sequences,\n\u001b[1;32m    118\u001b[0m         \u001b[38;5;66;03m# Set current unfinished to 1 if next token is not EOS\u001b[39;00m\n\u001b[1;32m    119\u001b[0m         next_tokens \u001b[38;5;241m!=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtokenizer\u001b[38;5;241m.\u001b[39meos_token_id,\n\u001b[1;32m    120\u001b[0m     )\n\u001b[1;32m    122\u001b[0m \u001b[38;5;66;03m# 5. Update intermediate artifacts\u001b[39;00m\n\u001b[0;32m--> 123\u001b[0m intermediate_model_log_prob \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mlog(model_prob)\n\u001b[1;32m    125\u001b[0m samples \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mcat([samples, next_tokens], dim\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m)\n\u001b[1;32m    126\u001b[0m \u001b[38;5;66;03m# ^Note: decoder-architectures will need the whole sequence at decoding time\u001b[39;00m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: Expected all tensors to be on the same device, but found at least two devices, cuda:0 and cpu!"
     ]
    }
   ],
   "source": [
    "from importance_sampler import ImportanceSampler\n",
    "isampler = ImportanceSampler(MODEL_NAME, device=\"cuda\")\n",
    "\n",
    "importance_probs, importance_samples = isampler.estimate(\n",
    "    input_str = \"I find \",\n",
    "    avoid_terms = \"I I am you he she her him they them my yours\",\n",
    "    num_sequences=NUM_SAMPLES,\n",
    "    max_num_tokens=MAX_TOKENS,\n",
    "    seed=SEED,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "338e39be",
   "metadata": {},
   "outputs": [],
   "source": [
    "hit_probs, _ = isampler.estimate_hit_probability()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a886990",
   "metadata": {},
   "outputs": [],
   "source": [
    "isampler.model.device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9b80a19",
   "metadata": {},
   "outputs": [],
   "source": [
    "hit_probs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef96bcd5",
   "metadata": {},
   "source": [
    "## Visualizations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5916513a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dcb5d0fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.barplot(x=np.arange(len(hit_probs)), y=np.array(hit_probs))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36c6a585",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_mean_std(array, stepsize=5):\n",
    "    means = []\n",
    "    stds = []\n",
    "    ids = []\n",
    "    \n",
    "    for i in range(5, len(array), stepsize):\n",
    "        mean = array[:i].mean().item()\n",
    "        means.append(mean)\n",
    "        \n",
    "        std = array[:i].std().item() / np.sqrt(i)\n",
    "        stds.append((mean-std, mean+std))\n",
    "        ids.append(i)\n",
    "    else:\n",
    "        mean = array.mean().item()\n",
    "        means.append(mean)\n",
    "        std = array.std().item() / np.sqrt(i)\n",
    "        stds.append((mean-std, mean+std))\n",
    "        ids.append(len(array))\n",
    "        \n",
    "    return means, stds, ids\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2fff35ab",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "d1cd5edf",
   "metadata": {},
   "source": [
    "$P(\\pi(K) = a) = P(X_K = a, X_{<K} \\neq a) = P(X_K = a| X_{<K} \\neq a) P(X_{<K} \\neq a)$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "123a8519",
   "metadata": {},
   "outputs": [],
   "source": [
    "input_str = \"Hi, my name is\"\n",
    "terms = \". . , ! ?\"\n",
    "imp_probs, imp_samples, imp_debug = timeit_sampling(\n",
    "    model, tokenizer, input_str, terms, num_samples, seed, max_num_tokens=MAX_TOKENS)\n",
    "compute_hitting_time_probabilities(imp_debug, terms, MAX_TOKENS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47055b29",
   "metadata": {},
   "outputs": [],
   "source": [
    "input_str = \"Where is\"\n",
    "terms = \"? ? ! , .\"\n",
    "imp_probs, imp_samples, imp_debug = timeit_sampling(model, tokenizer, input_str, terms, num_samples, seed, max_num_tokens=MAX_TOKENS)\n",
    "compute_hitting_time_probabilities(imp_debug, terms, MAX_TOKENS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1e6de5d",
   "metadata": {},
   "outputs": [],
   "source": [
    "input_str = \"Once upon a\"\n",
    "terms = \". ! . ... ,\"\n",
    "imp_probs, imp_samples, imp_debug = timeit_sampling(\n",
    "    model, tokenizer, input_str, terms, num_samples, seed, max_num_tokens=NUM_TOKENS)\n",
    "compute_hitting_time_probabilities(imp_debug, terms, MAX_TOKENS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d743c1b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "input_str = \"Paris is the capital of\"\n",
    "terms = \". . ...\"\n",
    "imp_probs, imp_samples, imp_debug = timeit_sampling(\n",
    "    model, tokenizer, input_str, terms, num_samples, seed, max_num_tokens=MAX_TOKENS)\n",
    "compute_hitting_time_probabilities(imp_debug, terms, MAX_TOKENS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dbed078f",
   "metadata": {},
   "outputs": [],
   "source": [
    "input_str = \"This is a story about a man and a hospital.\"\n",
    "terms = \". . ?\"\n",
    "imp_probs, imp_samples, imp_debug = timeit_sampling(\n",
    "    model, tokenizer, input_str, terms, num_samples, seed, max_num_tokens=MAX_TOKENS)\n",
    "compute_hitting_time_probabilities(imp_debug, terms, MAX_TOKENS)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "92cc5c84",
   "metadata": {},
   "source": [
    "### Examples for meeting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54df0fc7",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "input_str = \"The man who worked in the hospital\"\n",
    "terms = (\n",
    "    \" hospital\",\n",
    "    \" doctor\",\n",
    "    \" nurse\",\n",
    "    \" medical\",\n",
    "    \" medicine\",\n",
    "    \" emergency\", \n",
    "    \" hurt\", \n",
    "    \" caring\",\n",
    "    \" saved\",\n",
    "    \" surgeon\",\n",
    "    \" cleaning\",\n",
    ")\n",
    "\n",
    "for term in terms:\n",
    "    print(f\"\\n\\n ================== \\n TERM: {term} \\n ================= \\n\")\n",
    "    imp_probs, imp_samples, imp_debug = \\\n",
    "        timeit_sampling(model, tokenizer, input_str, term, num_samples, seed, max_num_tokens=MAX_TOKENS)\n",
    "    \n",
    "    compute_hitting_time_probabilities(imp_debug, term, MAX_TOKENS)\n",
    "    print(\"-> \" + \"\\n -> \".join(tokenizer.batch_decode(imp_samples[::5])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e889a5f",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "input_str = \"The woman who worked in the hospital\"\n",
    "terms = (\n",
    "    \" hospital\",\n",
    "    \" doctor\",\n",
    "    \" nurse\",\n",
    "    \" medical\",\n",
    "    \" medicine\",\n",
    "    \" emergency\", \n",
    "    \" hurt\", \n",
    "    \" caring\",\n",
    "    \" saved\",\n",
    "    \" surgeon\",\n",
    "    \" cleaning\",\n",
    ")\n",
    "\n",
    "for term in terms:\n",
    "    print(f\"\\n\\n ================== \\n TERM: {term} \\n ================= \\n\")\n",
    "    imp_probs, imp_samples, imp_debug = \\\n",
    "        timeit_sampling(model, tokenizer, input_str, term, num_samples, seed, max_num_tokens=NUM_TOKENS)\n",
    "    \n",
    "    compute_hitting_time_probabilities(imp_debug, term, NUM_TOKENS)\n",
    "    print(\"-> \" + \"\\n -> \".join(tokenizer.batch_decode(imp_samples[::5])))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d73f8f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "input_str = \"The man who worked in the hospital\"\n",
    "terms = \" \".join((\n",
    "    \" hospital\",\n",
    "    \" doctor\",\n",
    "    \" nurse\",\n",
    "    \" medical\",\n",
    "    \" medicine\",\n",
    "    \" emergency\", \n",
    "    \" hurt\", \n",
    "    \" caring\",\n",
    "    \" saved\",\n",
    "    \" surgeon\",\n",
    "    \" cleaning\",\n",
    "))\n",
    "\n",
    "print(f\"\\n\\n ================== \\n TERM: {terms} \\n ================= \\n\")\n",
    "imp_probs, imp_samples, imp_debug = \\\n",
    "    timeit_sampling(model, tokenizer, input_str, terms, num_samples, seed, max_num_tokens=NUM_TOKENS)\n",
    "\n",
    "compute_hitting_time_probabilities(imp_debug, terms, NUM_TOKENS)\n",
    "print(\"-> \" + \"\\n -> \".join(tokenizer.batch_decode(imp_samples[::5])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9087a571",
   "metadata": {},
   "outputs": [],
   "source": [
    "input_str = \"The woman who worked in the hospital\"\n",
    "terms = \" \".join((\n",
    "    \" hospital\",\n",
    "    \" doctor\",\n",
    "    \" nurse\",\n",
    "    \" medical\",\n",
    "    \" medicine\",\n",
    "    \" emergency\", \n",
    "    \" hurt\", \n",
    "    \" caring\",\n",
    "    \" saved\",\n",
    "    \" surgeon\",\n",
    "    \" cleaning\",\n",
    "))\n",
    "\n",
    "print(f\"\\n\\n ================== \\n TERM: {terms} \\n ================= \\n\")\n",
    "imp_probs, imp_samples, imp_debug = \\\n",
    "    timeit_sampling(model, tokenizer, input_str, terms, num_samples, seed, max_num_tokens=NUM_TOKENS)\n",
    "\n",
    "compute_hitting_time_probabilities(imp_debug, terms, NUM_TOKENS)\n",
    "print(\"-> \" + \"\\n -> \".join(tokenizer.batch_decode(imp_samples[::5])))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8d49372",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9b230c7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5819bc5e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "974e7c04",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f423cf20",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dfe789ed",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba22963f",
   "metadata": {},
   "outputs": [],
   "source": [
    "prob_not_occurring = []\n",
    "for i in range(MAX_TOKENS):\n",
    "    probs = imp_debug.get(i) \n",
    "    if probs is None: \n",
    "        break\n",
    "    \n",
    "    prob_not_occur = probs[\"intermediate_model_log_prob\"]\n",
    "    prob_not_occurring.append((1 - torch.exp(prob_not_occur)).mean().item())\n",
    "\n",
    "plt.figure(figsize=(8, 3))\n",
    "plt.title(f\"(Cumulative) probability of {terms} occurring at least once: P(N(k) > 0)\" )\n",
    "sns.barplot(x=np.arange(NUM_TOKENS)+1, y=prob_not_occurring)\n",
    "plt.xlabel(\"Number of tokens, K\")\n",
    "plt.ylabel(f\"P($N_{terms}$(k) > 0)\")\n",
    "sns.despine()\n",
    "plt.show()\n",
    "# TODO: What does this value mean?\n",
    "# P(t_1) * P(t_2 | t_1) * ... \n",
    "# It is the probability of the terms ocurring at position k, regardless of what has come before it.\n",
    "#prob_not_occurring_mass = np.array(prob_not_occurring)\n",
    "#prob_not_occurring_mass[1:] = prob_not_occurring_mass[1:] - prob_not_occurring_mass[:-1]\n",
    "#plt.figure(figsize=(10, 5))\n",
    "#plt.title(\"Mass probability of \\\"terms\\\" occurring at position k: P(N_terms(k))\" )\n",
    "#sns.barplot(x=np.arange(NUM_TOKENS)+1, y=prob_not_occurring_mass)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e308b825",
   "metadata": {},
   "outputs": [],
   "source": [
    "convergence_plot(input_str, avoid_terms, model=model, tokenizer=tokenizer, num_samples=num_samples, seeds=SEEDS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c36c9c5",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0adea281",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": true,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
