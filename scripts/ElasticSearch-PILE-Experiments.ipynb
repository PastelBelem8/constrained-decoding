{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3060b425",
   "metadata": {},
   "source": [
    "On 2023/01/26, we discussed it would be more interesting to analyze **conditional probabilities** (as opposed to unconditional probabilities), since language is largely contextual). To this end, we agreed on:\n",
    "\n",
    "- Run the analysis and comparisons in terms of tokens (as opposed to text).\n",
    "- Running **bi-gram** analysis of the words that co-occur more often within the beginning of the documents.\n",
    "- Use the first token of the bigram to condition the LM and observe the conditional probability of the second token, given the first one: $P(N_{w_2}(K) > 0|w_1)$\n",
    "\n",
    "Comparing the text and the set of tokens in terms of indices, mitigates issues with tokenization that arise in models like GPT2 or GPT-Neo, where **\"data\"** or **\" data\"** are represented differently. Note however, that we do not consider the probability mass (in our estimates) assigned to the sequence of tokens **\"d\" \"a\" \"t\" \"a\"**. We set to revisit this issue later if we find a big gap between model and data distributions.\n",
    "\n",
    "\n",
    "One pseudo-algorithm for computing the terms frequencies is:\n",
    "\n",
    "```\n",
    "Pseudocode: Compute the token frequencies of the first 100 tokens across all documents in the provided dataset.\n",
    "input: Tokenizer (T), docs (D)\n",
    "output: frequencies<int, int>\n",
    "\n",
    "frequencies<int, int> = dict()\n",
    "\n",
    "for d in D:\n",
    "  tokenized_doc = T.tokenize(d)\n",
    "  tokenized_doc = tokenized_doc.slice(100)  // get first 100 \n",
    "\n",
    "  frequencies.update_counts(tokenized_doc)\n",
    "end\n",
    "\n",
    "return frequencies\n",
    "```\n",
    "\n",
    "~~Implementation-wise, if we obtain a list of all possible document names, we can parallelize the terms counts.~~ However, chances are we cannot use the \"_id\" field, since it is a private field and we have no permissions to change the indexing mapping. Let us test the method on a couple of documents, if it's proven to be considerably fast, we will not parallelize it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b78a9787",
   "metadata": {},
   "outputs": [],
   "source": [
    "K_TOKENS = 10\n",
    "INDEX = \"re_pile\"\n",
    "\n",
    "# Default keyword arguments for elastic search\n",
    "default_kwargs = {\n",
    "    \"index\": INDEX,\n",
    "    \"track_total_hits\": True,\n",
    "}\n",
    "\n",
    "# Load elastic search \n",
    "from es_utils import load\n",
    "from compute_unigrams import read_yaml_config\n",
    "configs = read_yaml_config(\"./configs/elastic-search.yml\")\n",
    "es = load(**configs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "0254f202",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_tokenization_function(tokenizer, num_tokens: int):\n",
    "    from functools import partial\n",
    "    tokenizer.pad_token = tokenizer.pad_token or tokenizer.eos_token\n",
    "    \n",
    "    return partial(\n",
    "        tokenizer.batch_encode_plus,\n",
    "        max_length=num_tokens,\n",
    "        truncation=True,\n",
    "        padding=\"max_length\",\n",
    "        add_special_tokens=False,\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "aee51e59",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using pad_token, but it is not set yet.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Slow tokenizer: 50257\n",
      "Hello   world! [0, 5145]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using pad_token, but it is not set yet.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fast tokenizer: 50257\n",
      "Hello   world! [0, 5145]\n"
     ]
    }
   ],
   "source": [
    "num_tokens = 15\n",
    "model_name = \"EleutherAI/gpt-neo-125M\"\n",
    "\n",
    "from transformers import GPT2Tokenizer\n",
    "tokenizer_slow = GPT2Tokenizer.from_pretrained(model_name)\n",
    "tokenize_slow = get_tokenization_function(tokenizer_slow, num_tokens)\n",
    "print(\"Slow tokenizer:\", tokenizer_slow.vocab_size)\n",
    "\n",
    "\n",
    "from transformers import GPT2TokenizerFast\n",
    "tokenizer_fast = GPT2TokenizerFast.from_pretrained(model_name)\n",
    "tokenize_fast = get_tokenization_function(tokenizer_fast, num_tokens)\n",
    "print(\"Fast tokenizer:\", tokenizer_fast.vocab_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "dee80311",
   "metadata": {},
   "outputs": [],
   "source": [
    "import time as t\n",
    "import es_utils\n",
    "\n",
    "\n",
    "def test_scroll(tokenize: callable, query: dict, total_iters=20):\n",
    "    duration = []\n",
    "    \n",
    "    data = iter(es_utils.scroll(es, query, size=10_000, scroll=\"10m\", index=\"re_pile\"))\n",
    "\n",
    "    for i in range(total_iters):\n",
    "        docs = next(data)\n",
    "        \n",
    "        start = t.time()\n",
    "        tokenize([es_utils.get_text(d) for d in docs])\n",
    "        duration.append(t.time() - start)\n",
    "        print(duration)\n",
    "        \n",
    "    return duration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "f0c35d4b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total documents found {'value': 211036967, 'relation': 'eq'}\n",
      "[36.882235050201416]\n",
      "[36.882235050201416, 36.09491682052612]\n",
      "[36.882235050201416, 36.09491682052612, 33.985692739486694]\n",
      "[36.882235050201416, 36.09491682052612, 33.985692739486694, 38.33827185630798]\n",
      "[36.882235050201416, 36.09491682052612, 33.985692739486694, 38.33827185630798, 38.1004114151001]\n",
      "[36.882235050201416, 36.09491682052612, 33.985692739486694, 38.33827185630798, 38.1004114151001, 42.5689332485199]\n",
      "[36.882235050201416, 36.09491682052612, 33.985692739486694, 38.33827185630798, 38.1004114151001, 42.5689332485199, 39.27310395240784]\n",
      "[36.882235050201416, 36.09491682052612, 33.985692739486694, 38.33827185630798, 38.1004114151001, 42.5689332485199, 39.27310395240784, 31.29170274734497]\n",
      "[36.882235050201416, 36.09491682052612, 33.985692739486694, 38.33827185630798, 38.1004114151001, 42.5689332485199, 39.27310395240784, 31.29170274734497, 39.38059687614441]\n"
     ]
    }
]}
 ]
}