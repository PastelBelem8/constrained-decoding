{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "df8a2ab1",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sampling import naive_sampling\n",
    "from utils import set_seed, create_history, create_model_kwargs\n",
    "\n",
    "from transformers import GPT2Tokenizer, GPT2LMHeadModel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e4d78fd6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Freq: 0.1499999761581421 Var: 0.12814070284366608\n",
      "Produced samples\n",
      "Hello! Nice to see they have Burger King\n",
      "Hello! Nice to meet you! Good night\n",
      "Hello! Nice to see you comfortable and amazed\n",
      "Hello! Nice to meet you. Whats up\n",
      "Hello! Nice to see you, Izuna\n",
      "Hello! Nice to do this to you guys\n",
      "Hello! Nice to see you playing batting cages\n",
      "Hello! Nice to meet you all! Aunt\n",
      "Hello! Nice to meet you far from soon\n",
      "Hello! Nice to meet you-\" She said\n",
      "Hello! Nice to meet you me!\n",
      "\n",
      "\n",
      "Hello! Nice to greet you guys, now\n",
      "Hello! Nice to be here (very nice\n",
      "Hello! Nice to meet you too!\n",
      "\n",
      "Hello! Nice to meet you Miss. Not\n",
      "Hello! Nice to be with you connection!\"\n",
      "Hello! Nice to meet you!\n",
      "\n",
      "\n",
      "\n",
      "Hello! Nice to hear from you! Oh\n",
      "Hello! Nice to meet you! Welcome to\n",
      "Hello! Nice to see you many wares\n",
      "Hello! Nice to see you!\" She laughed\n",
      "Hello! Nice to meet you! But I\n",
      "Hello! Nice to meet you, Vidaho\n",
      "Hello! Nice to meet you Schr√∂d\n",
      "Hello! Nice to meet you!\" AnAg\n",
      "Hello! Nice to meet you guys!\n",
      "\n",
      "Hello! Nice to meet you!\"\n",
      "\n",
      "\n",
      "Hello! Nice to meet you guys. Please\n",
      "Hello! Nice to see you guys again!\n",
      "Hello! Nice to meet you and your friends\n",
      "Hello! Nice to see you on this branch\n",
      "Hello! Nice to meet you, your awesome\n",
      "Hello! Nice to see colleagues cleaning up our\n",
      "Hello! Nice to meet you!\n",
      "\n",
      "\n",
      "Hello! Nice to meet you first. I\n",
      "Hello! Nice to see you again!\n",
      "\n",
      "Hello! Nice to meet, because I'm\n",
      "Hello! Nice to meet you!\" Maddy\n",
      "Hello! Nice to meet you, I want\n",
      "Hello! Nice to see you here! My\n"
     ]
    }
   ],
   "source": [
    "# User definitions\n",
    "model_name = \"gpt2\"\n",
    "seed = 42\n",
    "\n",
    "num_samples = 200\n",
    "input_str = \"Hello! Nice to\"\n",
    "avoid_terms = \"meet you\"\n",
    "\n",
    "# ==========================================================\n",
    "# Load models\n",
    "tokenizer = GPT2Tokenizer.from_pretrained(model_name, model_max_length=512)\n",
    "model = GPT2LMHeadModel.from_pretrained(model_name, pad_token_id=tokenizer.eos_token_id)\n",
    "\n",
    "# Parse input and set seeds for reproducibility\n",
    "set_seed(seed)\n",
    "bos_token_id = tokenizer.bos_token_id or model.config.decoder_start_token_id\n",
    "\n",
    "input_ids = tokenizer(input_str, return_tensors=\"pt\", add_special_tokens=False).input_ids\n",
    "avoid_terms_ids = tokenizer(avoid_terms, add_special_tokens=False).input_ids\n",
    "\n",
    "# History (or past observations) and model_kwargs will be the same for all queries\n",
    "history = create_history(num_samples, input_ids, bos_token_id)\n",
    "\n",
    "# Call Naive Sampling\n",
    "mean, var, samples = naive_sampling(\n",
    "    avoid_term_ids=avoid_terms_ids,\n",
    "    **create_model_kwargs(history, model, tokenizer),\n",
    "    max_num_tokens=5,\n",
    "    model=model,\n",
    "    tokenizer=tokenizer,\n",
    ")\n",
    "\n",
    "print(\"Freq:\", mean, \"Var:\", var)\n",
    "print(\"Produced samples\")\n",
    "print(\"\\n\".join(tokenizer.batch_decode(samples)[::5]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "54340a9e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total #(occur set A): 170 (out of 200)\n",
      "Frequency #(occur set A) in samples 0.85\n",
      "Proba of not occurring: 0.15000000000000002\n"
     ]
    }
   ],
   "source": [
    "# naive counting of number of sentences w/ one of the avoid terms\n",
    "counts = 0\n",
    "\n",
    "for i, sample in enumerate(samples):\n",
    "    for token_id in sample:\n",
    "        if token_id in avoid_terms_ids:\n",
    "            #print(f\"'{tokenizer.decode(token_id)}' appeared in sample {i}: '{tokenizer.decode(sample)}'\")\n",
    "            counts+=1\n",
    "            break\n",
    "\n",
    "# \n",
    "print(\"Total #(occur set A):\", counts, f\"(out of {len(samples)})\")\n",
    "print(\"Frequency #(occur set A) in samples\", counts / len(samples))\n",
    "print(\"Proba of not occurring:\", 1 - counts / len(samples))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54d37ce8",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer.pad_token"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9499b82b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": true,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
