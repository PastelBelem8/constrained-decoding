{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3060b425",
   "metadata": {},
   "source": [
    "On 2023/01/26, we discussed it would be more interesting to analyze **conditional probabilities** (as opposed to unconditional probabilities), since language is largely contextual). To this end, we agreed on:\n",
    "\n",
    "- Run the analysis and comparisons in terms of tokens (as opposed to text).\n",
    "- Running **bi-gram** analysis of the words that co-occur more often within the beginning of the documents.\n",
    "- Use the first token of the bigram to condition the LM and observe the conditional probability of the second token, given the first one: $P(N_{w_2}(K) > 0|w_1)$\n",
    "\n",
    "Comparing the text and the set of tokens in terms of indices, mitigates issues with tokenization that arise in models like GPT2 or GPT-Neo, where **\"data\"** or **\" data\"** are represented differently. Note however, that we do not consider the probability mass (in our estimates) assigned to the sequence of tokens **\"d\" \"a\" \"t\" \"a\"**. We set to revisit this issue later if we find a big gap between model and data distributions.\n",
    "\n",
    "\n",
    "One pseudo-algorithm for computing the terms frequencies is:\n",
    "\n",
    "```\n",
    "Pseudocode: Compute the token frequencies of the first 100 tokens across all documents in the provided dataset.\n",
    "input: Tokenizer (T), docs (D)\n",
    "output: frequencies<int, int>\n",
    "\n",
    "frequencies<int, int> = dict()\n",
    "\n",
    "for d in D:\n",
    "  tokenized_doc = T.tokenize(d)\n",
    "  tokenized_doc = tokenized_doc.slice(100)  // get first 100 \n",
    "\n",
    "  frequencies.update_counts(tokenized_doc)\n",
    "end\n",
    "\n",
    "return frequencies\n",
    "```\n",
    "\n",
    "~~Implementation-wise, if we obtain a list of all possible document names, we can parallelize the terms counts.~~ However, chances are we cannot use the \"_id\" field, since it is a private field and we have no permissions to change the indexing mapping. Let us test the method on a couple of documents, if it's proven to be considerably fast, we will not parallelize it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b78a9787",
   "metadata": {},
   "outputs": [],
   "source": [
    "K_TOKENS = 10\n",
    "INDEX = \"re_pile\"\n",
    "\n",
    "# Default keyword arguments for elastic search\n",
    "default_kwargs = {\n",
    "    \"index\": INDEX,\n",
    "    \"track_total_hits\": True,\n",
    "}\n",
    "\n",
    "# Load elastic search \n",
    "from notebook_utils import load_elastic_search\n",
    "es = load_elastic_search(\"./configs/elastic-search.yml\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ddd5d341",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load tokenizer\n",
    "from transformers import GPT2Tokenizer\n",
    "tokenizer = GPT2Tokenizer.from_pretrained(\"EleutherAI/gpt-neo-125M\")\n",
    "tokenizer.pad_token = tokenizer.eos_token  # GPT2 does not have a pad token (EOS is used often)\n",
    "\n",
    "from functools import partial\n",
    "tokenize = partial(\n",
    "    tokenizer.batch_encode_plus,\n",
    "    max_length=K_TOKENS,\n",
    "    truncation=True,\n",
    "    padding=\"max_length\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "98f295ed",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total documents found {'value': 211036967, 'relation': 'eq'}\n",
      "Processing 10 documents\n"
     ]
    }
   ],
   "source": [
    "from notebook_utils import scroll\n",
    "\n",
    "query = {\"match_all\": {}}\n",
    "data = iter(scroll(es, query, size=10, **default_kwargs))\n",
    "docs = next(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "5cb7f5bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "from notebook_utils import filter_tokens\n",
    "from frequencies import PositionalFrequencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "764ad9ed",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing 10 documents\n",
      "0\n",
      "Processing 10 documents\n",
      "1\n",
      "Processing 10 documents\n",
      "2\n",
      "Processing 10 documents\n",
      "3\n",
      "Processing 10 documents\n",
      "4\n",
      "Processing 10 documents\n",
      "5\n",
      "Processing 10 documents\n",
      "6\n",
      "Processing 10 documents\n",
      "7\n",
      "Processing 10 documents\n",
      "8\n",
      "Processing 10 documents\n",
      "9\n"
     ]
    }
   ],
   "source": [
    "counts = PositionalFrequencies()\n",
    "j = 0\n",
    "# TODO: ? Create threads to process these results\n",
    "while docs and j<10:\n",
    "    results = tokenize(list(map(get_text, docs)))\n",
    "\n",
    "    input_ids = results[\"input_ids\"]\n",
    "    att_mask = results[\"attention_mask\"]\n",
    "\n",
    "    for inpt, attn in zip(input_ids, att_mask):\n",
    "        tokens = filter_tokens(inpt, attn)\n",
    "        for i, token in enumerate(tokens):\n",
    "            counts.add(token, i)\n",
    "    \n",
    "    docs = next(data)\n",
    "    print(j)\n",
    "    j += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "bea45ba9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(198, 5), (11, 3), (287, 3), (286, 2), (1944, 2)]"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "counts.most_common(5, 8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "060ecfb0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\n'"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.decode([198])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "0b9a0158",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "' of'"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.decode([286])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "9f69f6ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "import joblib\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "833b9b31",
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "Can't get attribute 'default_init' on <module 'frequencies' from '/home/kat/Projects/PhD/constrained-decoding/frequencies.py'>",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn [32], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mjoblib\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mload\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43munigram_counts.pkl\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/constrained-dec/lib/python3.9/site-packages/joblib/numpy_pickle.py:658\u001b[0m, in \u001b[0;36mload\u001b[0;34m(filename, mmap_mode)\u001b[0m\n\u001b[1;32m    652\u001b[0m             \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(fobj, \u001b[38;5;28mstr\u001b[39m):\n\u001b[1;32m    653\u001b[0m                 \u001b[38;5;66;03m# if the returned file object is a string, this means we\u001b[39;00m\n\u001b[1;32m    654\u001b[0m                 \u001b[38;5;66;03m# try to load a pickle file generated with an version of\u001b[39;00m\n\u001b[1;32m    655\u001b[0m                 \u001b[38;5;66;03m# Joblib so we load it with joblib compatibility function.\u001b[39;00m\n\u001b[1;32m    656\u001b[0m                 \u001b[38;5;28;01mreturn\u001b[39;00m load_compatibility(fobj)\n\u001b[0;32m--> 658\u001b[0m             obj \u001b[38;5;241m=\u001b[39m \u001b[43m_unpickle\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfobj\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfilename\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmmap_mode\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    659\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m obj\n",
      "File \u001b[0;32m~/miniconda3/envs/constrained-dec/lib/python3.9/site-packages/joblib/numpy_pickle.py:577\u001b[0m, in \u001b[0;36m_unpickle\u001b[0;34m(fobj, filename, mmap_mode)\u001b[0m\n\u001b[1;32m    575\u001b[0m obj \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    576\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 577\u001b[0m     obj \u001b[38;5;241m=\u001b[39m \u001b[43munpickler\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mload\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    578\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m unpickler\u001b[38;5;241m.\u001b[39mcompat_mode:\n\u001b[1;32m    579\u001b[0m         warnings\u001b[38;5;241m.\u001b[39mwarn(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mThe file \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m has been generated with a \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    580\u001b[0m                       \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mjoblib version less than 0.10. \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    581\u001b[0m                       \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mPlease regenerate this pickle file.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    582\u001b[0m                       \u001b[38;5;241m%\u001b[39m filename,\n\u001b[1;32m    583\u001b[0m                       \u001b[38;5;167;01mDeprecationWarning\u001b[39;00m, stacklevel\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m3\u001b[39m)\n",
      "File \u001b[0;32m~/miniconda3/envs/constrained-dec/lib/python3.9/pickle.py:1212\u001b[0m, in \u001b[0;36m_Unpickler.load\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1210\u001b[0m             \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mEOFError\u001b[39;00m\n\u001b[1;32m   1211\u001b[0m         \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(key, bytes_types)\n\u001b[0;32m-> 1212\u001b[0m         \u001b[43mdispatch\u001b[49m\u001b[43m[\u001b[49m\u001b[43mkey\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m]\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1213\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m _Stop \u001b[38;5;28;01mas\u001b[39;00m stopinst:\n\u001b[1;32m   1214\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m stopinst\u001b[38;5;241m.\u001b[39mvalue\n",
      "File \u001b[0;32m~/miniconda3/envs/constrained-dec/lib/python3.9/pickle.py:1537\u001b[0m, in \u001b[0;36m_Unpickler.load_stack_global\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1535\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mtype\u001b[39m(name) \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mstr\u001b[39m \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mtype\u001b[39m(module) \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mstr\u001b[39m:\n\u001b[1;32m   1536\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m UnpicklingError(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mSTACK_GLOBAL requires str\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m-> 1537\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mappend(\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfind_class\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodule\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mname\u001b[49m\u001b[43m)\u001b[49m)\n",
      "File \u001b[0;32m~/miniconda3/envs/constrained-dec/lib/python3.9/pickle.py:1581\u001b[0m, in \u001b[0;36m_Unpickler.find_class\u001b[0;34m(self, module, name)\u001b[0m\n\u001b[1;32m   1579\u001b[0m \u001b[38;5;28m__import__\u001b[39m(module, level\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0\u001b[39m)\n\u001b[1;32m   1580\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mproto \u001b[38;5;241m>\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m4\u001b[39m:\n\u001b[0;32m-> 1581\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_getattribute\u001b[49m\u001b[43m(\u001b[49m\u001b[43msys\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmodules\u001b[49m\u001b[43m[\u001b[49m\u001b[43mmodule\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mname\u001b[49m\u001b[43m)\u001b[49m[\u001b[38;5;241m0\u001b[39m]\n\u001b[1;32m   1582\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m   1583\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mgetattr\u001b[39m(sys\u001b[38;5;241m.\u001b[39mmodules[module], name)\n",
      "File \u001b[0;32m~/miniconda3/envs/constrained-dec/lib/python3.9/pickle.py:331\u001b[0m, in \u001b[0;36m_getattribute\u001b[0;34m(obj, name)\u001b[0m\n\u001b[1;32m    329\u001b[0m         obj \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mgetattr\u001b[39m(obj, subpath)\n\u001b[1;32m    330\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mAttributeError\u001b[39;00m:\n\u001b[0;32m--> 331\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mAttributeError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCan\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mt get attribute \u001b[39m\u001b[38;5;132;01m{!r}\u001b[39;00m\u001b[38;5;124m on \u001b[39m\u001b[38;5;132;01m{!r}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    332\u001b[0m                              \u001b[38;5;241m.\u001b[39mformat(name, obj)) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28mNone\u001b[39m\n\u001b[1;32m    333\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m obj, parent\n",
      "\u001b[0;31mAttributeError\u001b[0m: Can't get attribute 'default_init' on <module 'frequencies' from '/home/kat/Projects/PhD/constrained-decoding/frequencies.py'>"
     ]
    }
   ],
   "source": [
    "joblib.load(\"unigram_counts.pkl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d3fa237",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": true,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
