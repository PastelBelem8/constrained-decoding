{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "295ddb7c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd \n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "05f05f62",
   "metadata": {},
   "source": [
    "## Elastic Search\n",
    "\n",
    "Set up the engine and allow us to index the documents based on the documents that contain certain terms."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "076b1ab9",
   "metadata": {},
   "outputs": [],
   "source": [
    "from elasticsearch import Elasticsearch\n",
    "\n",
    "# Create an elastic search engine\n",
    "es = Elasticsearch(\n",
    "     cloud_id=\"lm-datasets:dXMtY2VudHJhbDEuZ2NwLmNsb3VkLmVzLmlvJDk1N2U5ODIwZDUxNTQ0YWViMjk0MmQwNzI1NjE0OTQ2JDhkN2M0OWMyZDEzMTRiNmM4NDNhNGEwN2U4NDE5NjRl\",\n",
    "     api_key=\"T2w5Vl9ZTUIzVzE5dTlBblUtRlo6MHNBYWxhbHVTeGFodUpUOWIybkNNZw==\",\n",
    "     retry_on_timeout=True,\n",
    "     http_compress=True,\n",
    ")\n",
    "\n",
    "def scroll_documents(es, query, size=50, scroll_time=\"20m\", index=\"re_pile\"):\n",
    "    data = es.search(index=index, query=query, size=size, scroll=scroll_time, sort=[\"_doc\"]) #TODO: Check score\n",
    "    hits, scroll_id = data[\"hits\"][\"hits\"], data[\"_scroll_id\"]\n",
    "    yield hits\n",
    "    \n",
    "    total = len(hits)\n",
    "    while len(hits) != 0:\n",
    "        data = es.scroll(scroll_id=scroll_id, scroll=scroll_time)\n",
    "        hits, scroll_id = data[\"hits\"][\"hits\"], data[\"_scroll_id\"]\n",
    "        total += len(hits)\n",
    "        yield hits\n",
    "    \n",
    "    es.clear_scroll(scroll_id=scroll_id)\n",
    "    print(f\"Done scrolling for query={query}!\")\n",
    "    yield None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "cd845882",
   "metadata": {},
   "outputs": [],
   "source": [
    "query = {'match': {'text': {'query': 'muslim angry', 'operator': 'and'}}}\n",
    "\n",
    "docs_iter = scroll_documents(es, query, size=10, index=\"re_pile\")\n",
    "while (docs := next(docs_iter)) is not None:\n",
    "    break\n",
    "    \n",
    "# Example\n",
    "text = docs[0][\"_source\"][\"text\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "6436f9f3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ObjectApiResponse({'count': 132451, '_shards': {'total': 20, 'successful': 20, 'skipped': 0, 'failed': 0}})"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "es.count(query=query, index=\"re_pile\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae2de7be",
   "metadata": {},
   "source": [
    "## Constraints\n",
    "\n",
    "Let us define simple RegexConstraint. This constraint will work in terms of looking for the exact match of the specified phrases you specify."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "156232f8",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/cbelem/miniconda3/envs/py39/lib/python3.9/site-packages/tqdm/auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "import spacy\n",
    "\n",
    "__nlp__ = spacy.load(\"en_core_web_sm\", disable=[\"ner\"])\n",
    "\n",
    "\n",
    "def get_phrases(text, phrases):\n",
    "    try:\n",
    "        return [text.index(p.lower()) + len(p) for p in phrases]\n",
    "    except:\n",
    "        return None\n",
    "    \n",
    "    \n",
    "class Constraint:\n",
    "    def __init__(self, *words, distance: int=30):\n",
    "        self.words = list(words)\n",
    "        self.wordsl = [p.lower() for p in self.words]\n",
    "\n",
    "        self.distance = distance\n",
    "        assert distance > 0\n",
    "        \n",
    "    @property\n",
    "    def es_query(self):\n",
    "        return {'match': {'text': {'query': \" \".join(self.words), 'operator': 'and'}}}\n",
    "    \n",
    "    def find_matches(self, text: str) -> list:\n",
    "        textl = text.lower()\n",
    "\n",
    "        # Indices \n",
    "        indices = get_phrases(textl, self.wordsl)\n",
    "\n",
    "        windows = []\n",
    "        for i in indices:\n",
    "            wstart = max(0, i-self.distance)\n",
    "            wend = min(len(text), i+self.distance+1)\n",
    "            text_i = textl[wstart:wend] \n",
    "            window_i = get_phrases(text_i, self.wordsl)\n",
    "            \n",
    "            if window_i is not None:\n",
    "                windows.append(text[wstart:wend])\n",
    "        \n",
    "        return windows\n",
    "\n",
    "    def get_prefix(self, window: str):\n",
    "        # Index returns the first occurrence of specified word\n",
    "        # We sum the length of the word w to obtain end character\n",
    "        prefixes = get_phrases(window.lower(), self.wordsl)\n",
    "        if prefixes is None:\n",
    "            print(\"Words:\\n->\", self.wordsl)\n",
    "            print(\"Skipping example:\\n->\", window)\n",
    "            return None, None\n",
    "        \n",
    "        # The largest prefix will definitely contain both words\n",
    "        prefixes = sorted(prefixes)\n",
    "        # We'll pick the longest prefix\n",
    "        prefix, continuation = window[:prefixes[-1]],  window[prefixes[-1]:]\n",
    "        \n",
    "        return prefix, continuation\n",
    "    \n",
    "    def get_minimal_prefix(self, prefix: str):\n",
    "        sentences = __nlp__(prefix).sents\n",
    "        sentences_ids = [prefix.index(s.text) for s in sentences]\n",
    "    \n",
    "        full_prefix = prefix\n",
    "        # Because of the way we create the prefixes we will\n",
    "        # prioritize right most prefix matching\n",
    "        sentences_ids = sentences_ids[::-1]\n",
    "        \n",
    "        for index in sentences_ids:\n",
    "            minimal_prefix = prefix[index:]\n",
    "            \n",
    "            # Check match of phrases\n",
    "            ids = get_phrases(minimal_prefix.lower(), self.wordsl)\n",
    "            if ids is not None:\n",
    "                return full_prefix, minimal_prefix\n",
    "        \n",
    "        return full_prefix, full_prefix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "47b5ae72",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--> Prefix: nk of 9/11, he totally deserves this\". When I read this, I was completely stunned. I even had to reread it again to see if I misread. Not only did I find it completely offensive towards me as a Muslim. I was dumbstruck on how such an unbelievable generalization that all Arabs (or anyone with a brown skin color for that matter) and Muslims are terrorist\n",
      "--> Full Prefix: nk of 9/11, he totally deserves this\". When I read this, I was completely stunned. I even had to reread it again to see if I misread. Not only did I find it completely offensive towards me as a Muslim. I was dumbstruck on how such an unbelievable generalization that all Arabs (or anyone with a brown skin color for that matter) and Muslims are terrorist\n",
      "--> Minimal prefix: I was dumbstruck on how such an unbelievable generalization that all Arabs (or anyone with a brown skin color for that matter) and Muslims are terrorist\n",
      "--> Continuation: s. I really had hoped that there would be a goo\n"
     ]
    }
   ],
   "source": [
    "# Obtain centered windows that satisfy constraints\n",
    "phrases = [\"muslim\", \"terrorist\"]\n",
    "constraint = Constraint(*phrases, distance=200)\n",
    "matches = constraint.find_matches(text)\n",
    "pref, cont = constraint.get_prefix(matches[0])\n",
    "full_pref, min_prefix = constraint.get_minimal_prefix(pref)\n",
    "\n",
    "print(\"--> Prefix:\", pref)\n",
    "print(\"--> Full Prefix:\", full_pref)\n",
    "print(\"--> Minimal prefix:\", min_prefix)\n",
    "\n",
    "print(\"--> Continuation:\", cont)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb45293f",
   "metadata": {},
   "source": [
    "### Sample N data sequences\n",
    "\n",
    "Sample N sequences from the training data that match different sequences. In particular, we will store the sequences in a data structure as follows:\n",
    "\n",
    "- `phrases::list[str]`: list of terms used to narrow down the sequences to process\n",
    "- `doc_id::str`: textual descriptor of the original document we sampled this from\n",
    "- `doc_subset::str`: textual descriptor of the data subset in PILE\n",
    "- `full_prefix::str`: text description of the full prefix \n",
    "- `min_prefix::str`: minimum set of sentences that satisfy the constraint.\n",
    "- `continuation::str`: continuation of the prefix\n",
    "\n",
    "Note that `min_prefix` should be a subset of the `full_prefix`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "05e40cca",
   "metadata": {},
   "outputs": [],
   "source": [
    "import traceback\n",
    "\n",
    "\n",
    "def sample_sequences(n_sequences, attribute, target, distance, scroll_size=100) -> pd.DataFrame:\n",
    "    phrases = [attribute, target]\n",
    "    constraint = Constraint(*phrases, distance=distance)\n",
    "\n",
    "    results = {\n",
    "        \"doc_id\": [],\n",
    "        \"doc_subset\": [],\n",
    "        \"full_prefix\": [],\n",
    "        \"min_prefix\": [],\n",
    "        \"continuation\": [],  \n",
    "    }\n",
    "\n",
    "    docs_iter = scroll_documents(es, constraint.es_query, size=scroll_size, index=\"re_pile\")\n",
    "\n",
    "    while (docs := next(docs_iter)) is not None and len(set(results[\"doc_id\"])) < n_sequences:\n",
    "\n",
    "        for doc in docs:\n",
    "            doc_id = doc[\"_id\"]\n",
    "            doc_subset = doc[\"_source\"][\"meta\"][\"pile_set_name\"]\n",
    "            text = doc[\"_source\"][\"text\"]\n",
    "\n",
    "            matches = constraint.find_matches(text)\n",
    "            \n",
    "            for match in matches:\n",
    "                try:\n",
    "                    pref, cont = constraint.get_prefix(match)\n",
    "                    if (pref, cont) == (None, None):\n",
    "                        continue\n",
    "\n",
    "                    full_prefix, min_prefix = constraint.get_minimal_prefix(pref)\n",
    "\n",
    "                    if len(min_prefix) < 5:\n",
    "                        print(f\"Oops! min_prefix '{min_prefix}' has less than 5 chars\")\n",
    "                        continue\n",
    "\n",
    "                    results[\"full_prefix\"].append(full_prefix)\n",
    "                    results[\"min_prefix\"].append(min_prefix)\n",
    "                    results[\"continuation\"].append(cont)\n",
    "\n",
    "                    results[\"doc_id\"].append(doc_id)\n",
    "                    results[\"doc_subset\"].append(doc_subset)    \n",
    "                except Exception:\n",
    "                    print(f\"Exception in user code: when processing: {match}\")\n",
    "                    print(\"-\"*60)\n",
    "                    traceback.print_exc(file=sys.stdout)\n",
    "                    print(\"-\"*60)\n",
    "    \n",
    "    results = pd.DataFrame(results)\n",
    "    results.insert(0, \"attribute\", [attribute] * len(results))\n",
    "    results.insert(0, \"target\", [target] * len(results))\n",
    "    return results    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9903b518",
   "metadata": {},
   "source": [
    "Let us define some set of terms to compare the generations from:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "6f2287c6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "16 8 128\n"
     ]
    }
   ],
   "source": [
    "target_words = [\n",
    "    \"muslim\",\n",
    "    \"jewish\",\n",
    "    \"buddhist\",\n",
    "    \"christian\",\n",
    "    \"doctor\",\n",
    "    \"nurse\",\n",
    "    \"mother\",\n",
    "    \"father\",\n",
    "    #\"african\",\n",
    "    #\"american\",\n",
    "    #\"caucasian\",\n",
    "    #\"asian\",\n",
    "    #\"indian\",\n",
    "]\n",
    "\n",
    "attribute_words = [\n",
    "    \"happy\",\n",
    "    \"sad\",\n",
    "    \"calm\",\n",
    "    \"angry\",\n",
    "    \"terror\",\n",
    "    \"peace\",\n",
    "    \"dead\",\n",
    "    \"death\",\n",
    "    \"great\",\n",
    "    \"good\",\n",
    "    \"bad\",\n",
    "    \"terrible\",\n",
    "    \"positive\",\n",
    "    \"negative\",\n",
    "    \"skill\",\n",
    "    \"food\",\n",
    "]\n",
    "\n",
    "print(len(attribute_words), len(target_words), len(attribute_words) * len(target_words))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "5cbff18c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "muslim happy 184137\n",
      "muslim sad 109348\n",
      "muslim calm 83826\n",
      "muslim angry 132451\n",
      "muslim terror 215362\n",
      "muslim peace 315972\n",
      "muslim dead 252341\n",
      "muslim death 392339\n",
      "muslim great 444824\n",
      "muslim good 499700\n",
      "muslim bad 276508\n",
      "muslim terrible 114336\n",
      "muslim positive 173353\n",
      "muslim negative 145802\n",
      "muslim skill 59412\n",
      "muslim food 224660\n",
      "jewish happy 260688\n",
      "jewish sad 159026\n",
      "jewish calm 123908\n",
      "jewish angry 164152\n",
      "jewish terror 180973\n",
      "jewish peace 355520\n",
      "jewish dead 313552\n",
      "jewish death 472622\n",
      "jewish great 577855\n",
      "jewish good 583981\n",
      "jewish bad 337094\n",
      "jewish terrible 176365\n",
      "jewish positive 216257\n",
      "jewish negative 179399\n",
      "jewish skill 102936\n",
      "jewish food 289324\n",
      "buddhist happy 88401\n",
      "buddhist sad 52134\n",
      "buddhist calm 54591\n",
      "buddhist angry 53344\n",
      "buddhist terror 39550\n",
      "buddhist peace 99839\n",
      "buddhist dead 88215\n",
      "buddhist death 128039\n",
      "buddhist great 170283\n",
      "buddhist good 168323\n",
      "buddhist bad 100441\n",
      "buddhist terrible 50546\n",
      "buddhist positive 75330\n",
      "buddhist negative 64814\n",
      "buddhist skill 43625\n",
      "buddhist food 103304\n",
      "christian happy 533176\n",
      "christian sad 304437\n",
      "christian calm 230829\n",
      "christian angry 301463\n",
      "christian terror 252838\n",
      "christian peace 547477\n",
      "christian dead 576258\n",
      "christian death 888105\n",
      "christian great 1220145\n",
      "christian good 1351628\n",
      "christian bad 700616\n",
      "christian terrible 313032\n",
      "christian positive 468729\n",
      "christian negative 365962\n",
      "christian skill 215954\n",
      "christian food 534498\n",
      "doctor happy 788822\n",
      "doctor sad 424184\n",
      "doctor calm 401167\n",
      "doctor angry 413851\n",
      "doctor terror 242025\n",
      "doctor peace 457489\n",
      "doctor dead 756828\n",
      "doctor death 1020713\n",
      "doctor great 1326531\n",
      "doctor good 1663924\n",
      "doctor bad 1025373\n",
      "doctor terrible 447383\n",
      "doctor positive 623788\n",
      "doctor negative 482429\n",
      "doctor skill 285605\n",
      "doctor food 812850\n",
      "nurse happy 337024\n",
      "nurse sad 214086\n",
      "nurse calm 209092\n",
      "nurse angry 210777\n",
      "nurse terror 135151\n",
      "nurse peace 221527\n",
      "nurse dead 318223\n",
      "nurse death 437158\n",
      "nurse great 504598\n",
      "nurse good 631522\n",
      "nurse bad 392257\n",
      "nurse terrible 211517\n",
      "nurse positive 331493\n",
      "nurse negative 244762\n",
      "nurse skill 166113\n",
      "nurse food 350766\n",
      "mother happy 1444798\n",
      "mother sad 731112\n",
      "mother calm 610345\n",
      "mother angry 713898\n",
      "mother terror 385095\n",
      "mother peace 867903\n",
      "mother dead 1399597\n",
      "mother death 2031023\n",
      "mother great 2525584\n",
      "mother good 2949854\n",
      "mother bad 1694018\n",
      "mother terrible 703405\n",
      "mother positive 959270\n",
      "mother negative 725640\n",
      "mother skill 409884\n",
      "mother food 1348591\n",
      "father happy 1327763\n",
      "father sad 685531\n",
      "father calm 576727\n",
      "father angry 692926\n",
      "father terror 401877\n",
      "father peace 929409\n",
      "father dead 1401970\n",
      "father death 2133760\n",
      "father great 2538195\n",
      "father good 2792476\n",
      "father bad 1598521\n",
      "father terrible 685016\n",
      "father positive 779231\n",
      "father negative 583195\n",
      "father skill 407174\n",
      "father food 1131649\n"
     ]
    }
   ],
   "source": [
    "import itertools as it\n",
    "\n",
    "all_results = []\n",
    "for target, attr in it.product(target_words, attribute_words):\n",
    "    query = {'match': {'text': {'query': f'{target} {attr}', 'operator': 'and'}}}\n",
    "    print(target, attr, es.count(index=\"re_pile\", query=query)[\"count\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c28ee43",
   "metadata": {},
   "source": [
    "### Let us start extracting the data for each individual group"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "7ab861f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "import os\n",
    "\n",
    "BASE_DIR = \"/extra/ucinlp1/cbelem/experiments-apr-15/data\"\n",
    "os.makedirs(BASE_DIR, exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "daf4cc30",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "  0%|                                                                                                                                                                                                                                                                                       | 0/7 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing filepath /extra/ucinlp1/cbelem/experiments-apr-15/data/jewish.csv\n",
      "======================================== happy ========================================\n",
      "======================================== sad ========================================\n",
      "======================================== calm ========================================\n",
      "======================================== angry ========================================\n",
      "Intermediate dump of results to /extra/ucinlp1/cbelem/experiments-apr-15/data/jewish.csv\n",
      "======================================== terror ========================================\n",
      "======================================== peace ========================================\n",
      "======================================== dead ========================================\n",
      "Words:\n",
      "-> ['dead', 'jewish']\n",
      "Skipping example:\n",
      "-> ead in an attack after they left Diyarbakır Police Department building. Radical Islamic group known as Turkish Hezbollah was suspected.\n",
      "\n",
      "Üzeyir Garih \n",
      " 25 August 2001: A prominent Turkish Jewish businessman and a founding partner of the Alarko group of companies.  He was stabbed to death  in the cemetery of the historic İstanbul quarter of Eyüp.\n",
      "\n",
      "Necip Hablemitoğlu \n",
      " 18 December 2002: A Kemalist hi\n",
      "======================================== death ========================================\n",
      "Intermediate dump of results to /extra/ucinlp1/cbelem/experiments-apr-15/data/jewish.csv\n",
      "======================================== great ========================================\n",
      "======================================== good ========================================\n",
      "======================================== bad ========================================\n",
      "======================================== terrible ========================================\n",
      "Intermediate dump of results to /extra/ucinlp1/cbelem/experiments-apr-15/data/jewish.csv\n",
      "======================================== positive ========================================\n",
      "======================================== negative ========================================\n",
      "======================================== skill ========================================\n",
      "======================================== food ========================================\n",
      "Words:\n",
      "-> ['food', 'jewish']\n",
      "Skipping example:\n",
      "-> d physician was always one of the sultan's Jewish subjects. Nearby, you can visit the late 17th-century **Kiosk of Kara Mustafa Pasha** (Sofa Köşkü), with its gilded ceiling, painted walls and delicate stained-glass windows. During the reign of Ahmet III, the Tulip Garden outside the kiosk was filled with the latest varieties of the flower.\n",
      "\n",
      "Up the stairs at the end of the Tulip Garden is the **Mar\n",
      "Intermediate dump of results to /extra/ucinlp1/cbelem/experiments-apr-15/data/jewish.csv\n",
      "Final result dump @ /extra/ucinlp1/cbelem/experiments-apr-15/data/jewish.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 14%|██████████████████████████████████████▏                                                                                                                                                                                                                                    | 1/7 [27:19<2:43:58, 1639.82s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing filepath /extra/ucinlp1/cbelem/experiments-apr-15/data/buddhist.csv\n",
      "======================================== happy ========================================\n",
      "======================================== sad ========================================\n",
      "======================================== calm ========================================\n",
      "======================================== angry ========================================\n",
      "Intermediate dump of results to /extra/ucinlp1/cbelem/experiments-apr-15/data/buddhist.csv\n",
      "======================================== terror ========================================\n",
      "======================================== peace ========================================\n",
      "======================================== dead ========================================\n",
      "======================================== death ========================================\n",
      "Intermediate dump of results to /extra/ucinlp1/cbelem/experiments-apr-15/data/buddhist.csv\n",
      "======================================== great ========================================\n",
      "======================================== good ========================================\n",
      "======================================== bad ========================================\n"
     ]
    }
   ],
   "source": [
    "N_SEQUENCES = 200\n",
    "CHAR_DISTANCE = 200\n",
    "\n",
    "\n",
    "for target in tqdm(target_words[1:]):\n",
    "    analysis_data = []\n",
    "    \n",
    "    filepath = f\"{BASE_DIR}/{target}.csv\"\n",
    "    print(\"Writing filepath\", filepath)\n",
    "    \n",
    "    for attr in attribute_words:\n",
    "        print(\"=\" * 40, attr, \"=\" * 40)\n",
    "        results = sample_sequences(N_SEQUENCES, attr, target, distance=CHAR_DISTANCE)\n",
    "        \n",
    "        if len(results) != 0:\n",
    "            analysis_data.append(results)\n",
    "        \n",
    "        if len(analysis_data) % 4 == 0:\n",
    "            print(\"Intermediate dump of results to\", filepath)\n",
    "            pd.concat(analysis_data).reset_index(drop=True).to_csv(filepath)\n",
    "        \n",
    "    print(\"Final result dump @\", filepath)\n",
    "    analysis_data = pd.concat(analysis_data).reset_index(drop=True)    \n",
    "    analysis_data.to_csv(filepath)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "477a92f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "N_SEQUENCES = 100\n",
    "CHAR_DISTANCE = 200\n",
    "\n",
    "for target in tqdm(target_words[1:]):\n",
    "    analysis_data = []\n",
    "    \n",
    "    for attr in [\"food\"]:\n",
    "        print(\"=\" * 40, attr, \"=\" * 40)\n",
    "        results = sample_sequences(N_SEQUENCES, attr, target, distance=CHAR_DISTANCE)\n",
    "        \n",
    "        if len(results) != 0:\n",
    "            analysis_data.append(results)\n",
    "            \n",
    "    analysis_data = pd.concat(analysis_data).reset_index(drop=True)\n",
    "    \n",
    "    \n",
    "    filepath = f\"{BASE_DIR}/{target}.csv\"\n",
    "    print(\"Writing filepath\", filepath)\n",
    "    analysis_data.to_csv(filepath)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a08a9ef0",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
