{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "6949a89e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "557a9b67",
   "metadata": {},
   "outputs": [],
   "source": [
    "BASE_DIR = \"/extra/ucinlp1/cbelem/experiments-apr-15\"\n",
    "\n",
    "OUTPUT_DIR = f\"{BASE_DIR}/toxicity_results\"\n",
    "os.makedirs(OUTPUT_DIR, exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "5a98189a",
   "metadata": {},
   "outputs": [],
   "source": [
    "TARGET_WORDS = [\n",
    "    \"muslim\",\n",
    "    \"jewish\",\n",
    "    \"buddhist\",\n",
    "    \"christian\",\n",
    "    \"doctor\",\n",
    "    \"nurse\",\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c6911dd",
   "metadata": {},
   "source": [
    "## Load Toxicity model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "77e6594a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/cbelem/miniconda3/envs/py39/lib/python3.9/site-packages/tqdm/auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.seq_relationship.weight', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.seq_relationship.bias', 'cls.predictions.bias', 'cls.predictions.transform.dense.bias', 'cls.predictions.decoder.weight']\n",
      "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[0.00338174, 0.00256998, 0.00332734, 0.00326782, 0.00316576,\n",
       "        0.00304676],\n",
       "       [0.01982095, 0.00048714, 0.00112874, 0.00064673, 0.00271987,\n",
       "        0.00104581]], dtype=float32)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from toxic_clf_model import ToxicCommentTagger\n",
    "import torch \n",
    "\n",
    "DEVICE = 1\n",
    "TOXIC_MODEL_DEVICE = f\"cuda:{DEVICE}\"\n",
    "# It is a multilabel model (we will consider only the \"toxic label\", index 0)\n",
    "# the other labels are as follow: toxic, severe_toxic, obscene, threat, insult, identity_hate\n",
    "toxicity_model = ToxicCommentTagger(n_classes=6)\n",
    "toxicity_model.load_state_dict(torch.load('./model.pth'))\n",
    "toxicity_model.eval();\n",
    "toxicity_model.to(TOXIC_MODEL_DEVICE)\n",
    "\n",
    "# sanity check\n",
    "toxicity_model.predict([\"Yo, ma homie! What's up?\", \"This is just a commmon sequence!\"], device=TOXIC_MODEL_DEVICE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "445cd276",
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_toxic_prediction(df: pd.DataFrame, sequences, colname: str, model, device, batch_size: int) -> pd.DataFrame:\n",
    "    df = df.copy()\n",
    "    result = model.predict(sequences, device=device, batch_size=batch_size)\n",
    "    \n",
    "    df[f\"{colname}toxicity\"]     = result[:,0]\n",
    "    df[f\"{colname}severe_toxic\"] = result[:,1]\n",
    "    df[f\"{colname}obscene\"]      = result[:,2]\n",
    "    df[f\"{colname}threat\"]       = result[:,3]\n",
    "    df[f\"{colname}insult\"]       = result[:,4]\n",
    "    df[f\"{colname}identity_hate\"]= result[:,5]\n",
    "    \n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f4b8c0d7",
   "metadata": {},
   "source": [
    "### Obtain toxicity for data"
   ]
  },
  {
   "cell_type": "raw",
   "id": "3779bb7d",
   "metadata": {},
   "source": [
    "DATA_DIR = f\"{BASE_DIR}/data\"\n",
    "DATA_OUT_DIR = f\"{OUTPUT_DIR}/data\"\n",
    "os.makedirs(DATA_OUT_DIR, exist_ok=True)\n",
    "\n",
    "TARGET_WORDS = [\n",
    "    \"muslim\",\n",
    "    \"jewish\",\n",
    "    \"buddhist\",\n",
    "    \"christian\",\n",
    "    # \n",
    "    \"doctor\",\n",
    "    \"nurse\",\n",
    "    \"mother\",\n",
    "    \"father\",\n",
    "    # \n",
    "]\n",
    "\n",
    "BASE_PATHS = [f\"{DATA_DIR}/{t}.csv\" for t in TARGET_WORDS]\n",
    "BASE_DATA = [pd.read_csv(p, index_col=0) for p in BASE_PATHS]"
   ]
  },
  {
   "cell_type": "raw",
   "id": "9c8d8ba7",
   "metadata": {},
   "source": [
    "prefix_col, suffix_col = \"min_prefix\", \"continuation\"\n",
    "\n",
    "for target, data in zip(TARGET_WORDS, BASE_DATA):\n",
    "    data = data.dropna()\n",
    "    sequences = (data[prefix_col] + data[suffix_col]).values.tolist()\n",
    "    sequences_preds = add_toxic_prediction(data, sequences, \"\", toxicity_model, batch_size=32, device=TOXIC_MODEL_DEVICE)\n",
    "    sequences_preds.to_csv(f\"{DATA_OUT_DIR}/{target}.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7820052c",
   "metadata": {},
   "source": [
    "### Obtain toxicity model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "6967fa13",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "christian\n"
     ]
    }
   ],
   "source": [
    "TARGET_WORD = \"christian\"\n",
    "print(TARGET_WORD)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "3f529c80",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Read data from /extra/ucinlp1/cbelem/experiments-apr-15/models/EleutherAI__pythia-70m/christian_min_prefix.csv 2662400\n"
     ]
    }
   ],
   "source": [
    "MODEL_DIR = f\"{BASE_DIR}/models/EleutherAI__pythia-70m\"\n",
    "MODEL_OUT_DIR = f\"{OUTPUT_DIR}/models/EleutherAI__pythia-70m\"\n",
    "os.makedirs(MODEL_OUT_DIR, exist_ok=True)\n",
    "\n",
    "BASE_PATH = f\"{MODEL_DIR}/{TARGET_WORD}_min_prefix.csv\"\n",
    "BASE_DATA = pd.read_csv(BASE_PATH, index_col=0)\n",
    "print(\"Read data from\", BASE_PATH, len(BASE_DATA))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "e3ef2aa4",
   "metadata": {},
   "outputs": [],
   "source": [
    "top_p_mask = BASE_DATA[\"sampling_kwargs\"].apply(lambda x: \"top_p\" in x)\n",
    "BASE_DATA.loc[top_p_mask, \"sampling\"] = [\"top-p\"] * sum(top_p_mask)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "b39d973e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>prefix</th>\n",
       "      <th>sequence</th>\n",
       "      <th>sequence_log_prob</th>\n",
       "      <th>sampling_kwargs</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>sampling</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>multinomial</th>\n",
       "      <td>204800</td>\n",
       "      <td>204800</td>\n",
       "      <td>204800</td>\n",
       "      <td>204800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>temperature</th>\n",
       "      <td>819200</td>\n",
       "      <td>819200</td>\n",
       "      <td>819200</td>\n",
       "      <td>819200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>top-k</th>\n",
       "      <td>819200</td>\n",
       "      <td>819200</td>\n",
       "      <td>819200</td>\n",
       "      <td>819200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>top-p</th>\n",
       "      <td>819200</td>\n",
       "      <td>819200</td>\n",
       "      <td>819200</td>\n",
       "      <td>819200</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "             prefix  sequence  sequence_log_prob  sampling_kwargs\n",
       "sampling                                                         \n",
       "multinomial  204800    204800             204800           204800\n",
       "temperature  819200    819200             819200           819200\n",
       "top-k        819200    819200             819200           819200\n",
       "top-p        819200    819200             819200           819200"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# sanity check \n",
    "BASE_DATA.groupby(\"sampling\").count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "8a273c85",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['multinomial', 'temperature', 'top-k', 'top-p']"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Let us process the toxicity by parts\n",
    "sampling_types = sorted(BASE_DATA[\"sampling\"].unique())\n",
    "sampling_types"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "87e8886e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['multinomial', 'temperature']\n",
      "multinomial christian\n",
      "temperature christian\n"
     ]
    }
   ],
   "source": [
    "print(sampling_types[:2])\n",
    "for sampling in sampling_types[:2]:\n",
    "    print(sampling, TARGET_WORD)\n",
    "    data = BASE_DATA[BASE_DATA[\"sampling\"] == sampling]\n",
    "    data = data.dropna()\n",
    "    sequences = (data[\"sequence\"]).values.tolist()\n",
    "    sequences_preds = add_toxic_prediction(data, sequences, \"\", toxicity_model, batch_size=16, device=TOXIC_MODEL_DEVICE)\n",
    "    sequences_preds.to_csv(f\"{MODEL_OUT_DIR}/{TARGET_WORD}_{sampling}_pt1.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "afa1bc58",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
