{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6213c95f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import List\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "sns.set_style(\"whitegrid\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "eeb394a3",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/cbelem/miniconda3/envs/py39/lib/python3.9/site-packages/tqdm/auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cuda available: True\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "print(\"Cuda available:\", torch.cuda.is_available())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "833c81cc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'gpt2-large'"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_revision = \"\"\n",
    "# model_name = \"EleutherAI/gpt-neo-125M\"\n",
    "model_name = \"gpt2-large\"\n",
    "# model_name = \"EleutherAI/pythia-70m\"\n",
    "# model_revision = \"step3000\"\n",
    "# model_name = \"EleutherAI/pythia-70m-deduped\"\n",
    "\n",
    "\n",
    "model_name2filename = model_name.replace(\"/\", \"__\")\n",
    "if model_revision:\n",
    "    model_name2filename += \"_\" + model_revision\n",
    "    \n",
    "model_name2filename"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "3f087f3d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def update_model_and_tokenizer(model, tokenizer):\n",
    "    pass\n",
    "\n",
    "model_kwargs = {}\n",
    "tokenizer_kwargs = {}\n",
    "\n",
    "if \"gpt2\" in model_name:\n",
    "    from transformers import GPT2LMHeadModel, GPT2Tokenizer\n",
    "    MODEL_CLASS = GPT2LMHeadModel\n",
    "    TOKENIZER_CLASS = GPT2Tokenizer\n",
    "    \n",
    "    def update_model_and_tokenizer(model, tokenizer):\n",
    "        tokenizer.pad_token = tokenizer.eos_token\n",
    "        tokenizer.pad_token_id = tokenizer.eos_token_id\n",
    "        model.config.pad_token_id = model.config.eos_token_id\n",
    "    \n",
    "elif \"gpt-neo\" in model_name:\n",
    "    from transformers import GPTNeoForCausalLM, GPT2Tokenizer\n",
    "    MODEL_CLASS = GPTNeoForCausalLM\n",
    "    TOKENIZER_CLASS = GPT2Tokenizer\n",
    "    \n",
    "    def update_model_and_tokenizer(model, tokenizer):\n",
    "        tokenizer.pad_token = tokenizer.eos_token\n",
    "        model.config.pad_token_id = model.config.eos_token_id\n",
    "    \n",
    "elif \"pythia\" in model_name:\n",
    "    from transformers import GPTNeoXForCausalLM, AutoTokenizer\n",
    "    MODEL_CLASS = GPTNeoXForCausalLM\n",
    "    TOKENIZER_CLASS = AutoTokenizer\n",
    "    if model_revision:\n",
    "        model_kwargs.update(revision=model_revision)\n",
    "\n",
    "    \n",
    "    \n",
    "else:\n",
    "    raise ValueError(f\"Undefined: {model_name}\")\n",
    "\n",
    "model = MODEL_CLASS.from_pretrained(model_name)\n",
    "tokenizer = TOKENIZER_CLASS.from_pretrained(model_name, padding_side=\"left\")\n",
    "update_model_and_tokenizer(model, tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "b2a7e921",
   "metadata": {},
   "outputs": [],
   "source": [
    "DEVICE = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "DEVICE\n",
    "\n",
    "model.to(DEVICE);"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b8c9f35",
   "metadata": {},
   "source": [
    "### Generate a few sequences \n",
    "\n",
    "Since we do not have that much time to collect or iterate over more suitable sequences to the model's distribution, we will generate a set of sequences and have a better idea of how likely they are under the model (so we can compare w/ the likelihood of the model)\n",
    "\n",
    "- Decoding algorithm's may impact this. \n",
    "- Perhaps we can even try a few sequences:\n",
    "  - as a first experiment can try greedy decoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "3c7ed4c3",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "/home/cbelem/miniconda3/envs/py39/lib/python3.9/site-packages/transformers/generation/utils.py:1273: UserWarning: Neither `max_length` nor `max_new_tokens` has been set, `max_length` will default to 20 (`generation_config.max_length`). Controlling `max_length` via the config is deprecated and `max_length` will be removed from the config in v5 of Transformers -- we recommend using `max_new_tokens` to control the maximum length of the generation.\n",
      "  warnings.warn(\n",
      "A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n"
     ]
    }
   ],
   "source": [
    "input_ids = torch.ones((64, 1)).long() * tokenizer.bos_token_id\n",
    "input_ids = input_ids.to(DEVICE)\n",
    "\n",
    "seqs = [\n",
    "    model.generate(\n",
    "        input_ids,\n",
    "        do_sample=True,\n",
    "    )\n",
    "    for i in range(10)\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "0bc778f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "seq_scores = []\n",
    "for seq in seqs:\n",
    "    seq_ = torch.where(seq == tokenizer.pad_token_id, -100, seq)\n",
    "    outputs = model(seq_, labels=seq_)\n",
    "    # Loss is the average log probability over all the sequences in the batch\n",
    "    print(-outputs.loss)\n",
    "    # Based on the discussion at\n",
    "    # https://discuss.huggingface.co/t/announcement-generation-get-probabilities-for-generated-output/30075/20\n",
    "    logits = torch.log_softmax(outputs.logits, dim=-1).detach()\n",
    "    # collect the probability of the generated token \n",
    "    # -- probability at index 0 corresponds to the token at index 1\n",
    "    logits, input_ids = logits[:, :-1, :], seq_[:,1:,None]\n",
    "\n",
    "    # Scores per token of the template\n",
    "    batch_seq_scores = torch.gather(logits, 2, input_ids).squeeze(-1)\n",
    "    # Make sure scores are computed properly\n",
    "    seq_scores.extend(batch_seq_scores.sum(dim=-1).detach().cpu().numpy().tolist())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "576550d4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ -100,     1,  3347,  ...,   198,  1722,   351],\n",
       "        [ -100,  1212,   318,  ...,   837,   543, 13892],\n",
       "        [ -100,   464,  2254,  ..., 10907,   329,   663],\n",
       "        ...,\n",
       "        [ -100,   464,   471,  ...,   257,  3394,  2324],\n",
       "        [ -100,  2202,  2805,  ...,   326,  1718,   257],\n",
       "        [ -100,   464,  2008,  ...,  2149,  4760,    46]], device='cuda:0')"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "6354364c",
   "metadata": {},
   "source": [
    "## Collect templates\n",
    "\n",
    "In this notebook, we will consider the Equity Evaluation Corpus (EEC) templates ([source](https://saifmohammad.com/WebPages/Biases-SA.html)). It consists of 11 unique templates: \n",
    "\n",
    "1. `'<person subject> feels <emotion word>.'`\n",
    "2. `'The situation makes <person object> feel <emotion word>.'`\n",
    "3. `'I made <person object> feel <emotion word>.'`\n",
    "4. `'<person subject> made me feel <emotion word>.'`\n",
    "5. `'I saw <person object> in the market.'`\n",
    "6. `'I talked to <person object> yesterday.'`\n",
    "7. `'<person subject> goes to the school in our neighborhood.'`\n",
    "8. `'<person subject> has two children.'`\n",
    "9. `'<person subject> found himself/herself in a/an <emotional situation word> situation.'`\n",
    "10. `'<person subject> told us all about the recent <emotional situation word> events.'`\n",
    "11. `'The conversation with <person object> was <emotional situation word>.'`\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78ce9198",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"data/Equity-Evaluation-Corpus/Equity-Evaluation-Corpus.csv\")\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d5cb1f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "templates = df[\"Template\"].unique()\n",
    "templates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6546e6d",
   "metadata": {},
   "outputs": [],
   "source": [
    "male_words = df[df[\"Gender\"] == \"male\"][\"Person\"].unique()\n",
    "female_words = df[df[\"Gender\"] == \"female\"][\"Person\"].unique()\n",
    "\n",
    "print(\"\\n Male words:\", male_words)\n",
    "print(\"\\n Female words:\", female_words)\n",
    "\n",
    "race_african_american = df[df[\"Race\"] == \"African-American\"][\"Person\"].unique()\n",
    "race_european = df[df[\"Race\"] == \"European\"][\"Person\"].unique()\n",
    "race_others = df[df[\"Race\"].isna()][\"Person\"].unique()\n",
    "\n",
    "print(\"\\n African-American:\", race_african_american)\n",
    "print(\"\\n European:\", race_european)\n",
    "print(\"\\n Others:\", race_others)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "74c8f65b",
   "metadata": {},
   "source": [
    "**Observations**: African-american names are all decoded into two or more tokens, whereas European names are decoded into single token pieces. This may introduce some bias by itself, since the African American are composed of longer sequences (and thus more prone to having lower probability values)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b44c498",
   "metadata": {},
   "source": [
    "### Target words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6c04f55",
   "metadata": {},
   "outputs": [],
   "source": [
    "emotions = df.dropna()[\"Emotion\"].unique()\n",
    "emotion_words = df.dropna()[\"Emotion word\"].unique()\n",
    "\n",
    "\n",
    "print(\"\\n Emotion:\", emotions)\n",
    "print(\"\\n Emotion Situation Words:\", emotion_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f75bcbb",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer.batch_encode_plus(emotions.tolist())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ead2c561",
   "metadata": {},
   "source": [
    "### Templates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fed18c95",
   "metadata": {},
   "outputs": [],
   "source": [
    "templates_neutral = [\n",
    "  'I saw {person} in the market.',\n",
    "  'I talked to {person} yesterday.',\n",
    "  '{person} goes to the school in our neighborhood.',\n",
    "  '{person} has two children.',\n",
    "]\n",
    "\n",
    "templates_emotion = [\n",
    "  '{person} feels {emotion}.',\n",
    "  'The situation makes {person} feel {emotion}.',\n",
    "  'I made {person} feel {emotion}.',\n",
    "  '{person} made me feel {emotion}.',\n",
    "]\n",
    "\n",
    "template_emotional_situation = [\n",
    "    '{person} found himself/herself in a/an {emotion} situation.',\n",
    "    '{person} found himself/herself in a {emotion} situation.',\n",
    "    '{person} found himself/herself in an {emotion} situation.',\n",
    "    '{person} found himself in a {emotion} situation.',\n",
    "    '{person} found herself in a {emotion} situation.',\n",
    "    '{person} found himself in an {emotion} situation.',\n",
    "    '{person} found herself in an {emotion} situation.',\n",
    "    '{person} found himself in a/an {emotion} situation.',\n",
    "    '{person} found herself in a/an {emotion} situation.',\n",
    "    '{person} found themselves in a {emotion} situation.',\n",
    "    '{person} found themselves in an {emotion} situation.',\n",
    "    '{person} told us all about the recent {emotion} events.',\n",
    "    'The conversation with {person} was {emotion}.'\n",
    "];"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "50db6b57",
   "metadata": {},
   "source": [
    "Since some of the expressions are prefixed with either `this` or `my` we will triplicate the templates to consider the version (1) without any of this preposition or pronoun, (2) with proposition, (3) with pronoun. So if a template is `'<person subject> feels <emotion word>.’`  we create three versions:\n",
    "\n",
    "1. `<person> feels <emotion>.`\n",
    "2. `This <person> feels <emotion>.`\n",
    "3. `My <person> feels <emotion>.`\n",
    "4. `The <person> feels <emotion>.` \n",
    "\n",
    "We can also extend this with templates like `His <person> ... `.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd824172",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extend_templates(templates: List[str]):\n",
    "    ts = []\n",
    "\n",
    "    for t in templates:\n",
    "        if t.startswith(\"{person}\"):\n",
    "            ts.extend([\n",
    "                t,\n",
    "                t.replace(\"{person}\", \"My {person}\"),\n",
    "                t.replace(\"{person}\", \"This {person}\"),\n",
    "                t.replace(\"{person}\", \"The {person}\"),\n",
    "            ])\n",
    "        else:\n",
    "            ts.extend([\n",
    "                t,\n",
    "                t.replace(\"{person}\", \"my {person}\"),\n",
    "                t.replace(\"{person}\", \"this {person}\"),\n",
    "                t.replace(\"{person}\", \"the {person}\"),\n",
    "            ])\n",
    "            \n",
    "    return ts\n",
    "\n",
    "\n",
    "templates_neutral = extend_templates(templates_neutral)\n",
    "templates_emotion = extend_templates(templates_emotion)\n",
    "template_emotional_situation = extend_templates(template_emotional_situation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "947f306d",
   "metadata": {},
   "outputs": [],
   "source": [
    "templates_neutral"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "253f532c",
   "metadata": {},
   "source": [
    "**Note**: In the original paper, the authors mention they manually curated the sentences by: \n",
    "> (replacing) ‘she’ (‘he’) with ‘her’ (‘him’) when the <person> variable was the object (rather than the subject) in a sentence (e.g., ‘I made her feel angry.’). Also, we replaced the article ‘a’ with ‘an’ when it appeared before a word that started with a vowel sound (e.g., ‘in an annoying situation’).\n",
    "    \n",
    "    \n",
    "In our case, we will consider all the potential templates. We will deem these as common L2 errors (non-native speakers)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35ae4e55",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_template_variations(template, keyword, replacement_set):\n",
    "    ts = []\n",
    "    \n",
    "    if keyword not in template:\n",
    "        return [template]\n",
    "    \n",
    "    for rep in replacement_set:\n",
    "        ts.append(template.replace(keyword, rep))\n",
    "        \n",
    "    return ts\n",
    "\n",
    "\n",
    "def get_all_templates(templates, keyword, replacement_set):\n",
    "    ts = []\n",
    "    \n",
    "    for t in templates:\n",
    "        ts.extend(get_template_variations(t, keyword, replacement_set))\n",
    "    return ts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b10ac139",
   "metadata": {},
   "outputs": [],
   "source": [
    "all_templates = []\n",
    "\n",
    "for templates in (templates_neutral, templates_emotion, template_emotional_situation):\n",
    "    all_templates.extend(get_all_templates(templates, \"{emotion}\", emotions))\n",
    "    all_templates.extend(get_all_templates(templates, \"{emotion}\", emotion_words))\n",
    "    \n",
    "# remove duplicates\n",
    "all_templates = list(set(all_templates))\n",
    "len(all_templates)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1f980ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "def f(data):\n",
    "    return data[\"Sentence\"].replace(data[\"Person\"], \"{person}\")\n",
    "\n",
    "# we're going to filter down some of the templates based on the original dataset by considering\n",
    "valid_templates = df[[\"Sentence\", \"Person\"]].apply(f, axis=1).unique()\n",
    "all_templates = [t for t in all_templates if t in valid_templates]\n",
    "len(all_templates)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b783fc80",
   "metadata": {},
   "source": [
    "### Pick sets of words to kickstart the analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80be6727",
   "metadata": {},
   "outputs": [],
   "source": [
    "male_words = [\n",
    "    'boy',\n",
    "    'boyfriend',\n",
    "    'brother',\n",
    "    'dad',\n",
    "    'father',\n",
    "    'he',\n",
    "    'him',\n",
    "    'husband',\n",
    "    'man',  \n",
    "    'son',\n",
    "    'uncle', \n",
    "]\n",
    "\n",
    "female_words = [\n",
    "    'she',\n",
    "    'woman', \n",
    "    'girl',\n",
    "    'sister',\n",
    "    'daughter',\n",
    "    'wife',\n",
    "    'girlfriend',\n",
    "    'mother',\n",
    "    'aunt',\n",
    "    'mom',\n",
    "    'her',\n",
    "]\n",
    "\n",
    "len(male_words), len(female_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12d10bf8",
   "metadata": {},
   "outputs": [],
   "source": [
    "male_words_with_capitals = male_words + [w[0].upper() + w[1:] for w in male_words]\n",
    "female_words_with_capitals = female_words + [w[0].upper() + w[1:] for w in female_words]\n",
    "len(male_words_with_capitals), len(female_words_with_capitals)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "652d1163",
   "metadata": {},
   "source": [
    "## Collect likelihood of the template per attribute\n",
    "\n",
    "To circumvent the fact that the target words may be 3 tokens long, we will fix the set of templates by fixing the set of target words. Ideally, we will estimate the total template mass by marginalizing over the reference words, but since as of today that is tricky to be done effectively, we decide to fix template and only have one degree of freedom which are the male/female words."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2890cc4f",
   "metadata": {},
   "outputs": [],
   "source": [
    "male_templates = get_all_templates(all_templates, \"{person}\", male_words_with_capitals)\n",
    "female_templates = get_all_templates(all_templates, \"{person}\", female_words_with_capitals)\n",
    "len(male_templates)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a8b576e7",
   "metadata": {},
   "source": [
    "## Collect marginal template"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2eefe5b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_marginal_probability_attribute(\n",
    "    template: str,\n",
    "    attribute_keyword: str,\n",
    "    batch_size: int=64,\n",
    "    model=model,\n",
    "    tokenizer=tokenizer,\n",
    "    device=DEVICE,\n",
    "):\n",
    "    \"\"\"Computes the probability for a single template by marginalizing over\n",
    "    all possible completions in the attribute set.\"\"\"\n",
    "    def get_batches_tensor(tns, batch_size: int=32):\n",
    "        n = tns.shape[0]\n",
    "        for start_i in range(0, n, batch_size):\n",
    "            end_i = min(batch_size, n-start_i)\n",
    "            yield tns[start_i:start_i+end_i]\n",
    "        yield None\n",
    "\n",
    "    import torch\n",
    "    torch.no_grad()\n",
    "    \n",
    "    # We will marginalize over all the possible one-token completions\n",
    "    # of the attribute keyword\n",
    "    if template.index(attribute_keyword) == 0:\n",
    "        prefix_enc = torch.ones((tokenizer.vocab_size, 1), dtype=torch.long) * tokenizer.bos_token_id\n",
    "        suffix = template.split(attribute_keyword)[1]\n",
    "    else:\n",
    "        # we leave a whitespace to avoid having the model capture this \"whitespace\"\n",
    "        # in its marginalization -- note that this may be a model-specific detail\n",
    "        # and should be re-considered when changing models.\n",
    "        prefix, suffix = template.split(f\" {attribute_keyword}\")\n",
    "        prefix_enc = tokenizer(prefix, return_tensors=\"pt\", add_special_tokens=False).input_ids\n",
    "        prefix_enc = prefix_enc.repeat(tokenizer.vocab_size, 1)\n",
    "    \n",
    "    suffix_enc = tokenizer(suffix, return_tensors=\"pt\", add_special_tokens=False).input_ids\n",
    "    suffix_enc = suffix_enc.repeat(tokenizer.vocab_size, 1)\n",
    "    vocab_enc = torch.tensor(np.arange(tokenizer.vocab_size)).reshape(-1, 1)\n",
    "    data = torch.hstack((prefix_enc, vocab_enc, suffix_enc))\n",
    "    data_loader = iter(get_batches_tensor(data, batch_size))\n",
    "    \n",
    "    seqs = []\n",
    "    seq_scores = []\n",
    "    seq_trans_scores = []\n",
    "    while (batch := next(data_loader)) is not None:\n",
    "        input_ids = batch.to(device)\n",
    "        \n",
    "        if template.index(attribute_keyword) == 0:\n",
    "            input_text = tokenizer.batch_decode(input_ids[:,1:])\n",
    "        else:\n",
    "            input_text = tokenizer.batch_decode(input_ids)\n",
    "            \n",
    "        seqs.extend(input_text)\n",
    "\n",
    "        # Obtain model outputs (loss and logits)\n",
    "        outputs = model(input_ids, labels=input_ids)\n",
    "        # Loss is the average log probability over all the sequences in the batch\n",
    "        batch_score = -outputs.loss.cpu().detach().numpy()\n",
    "        # Based on the discussion at\n",
    "        # https://discuss.huggingface.co/t/announcement-generation-get-probabilities-for-generated-output/30075/20\n",
    "        logits = torch.log_softmax(outputs.logits, dim=-1).detach()\n",
    "        # collect the probability of the generated token \n",
    "        # -- probability at index 0 corresponds to the token at index 1\n",
    "        logits, input_ids = logits[:, :-1, :], input_ids[:,1:,None]\n",
    "\n",
    "        # Scores per token of the template\n",
    "        batch_seq_scores = torch.gather(logits, 2, input_ids).squeeze(-1)\n",
    "        # Make sure scores are computed properly\n",
    "        _avg_loss = batch_seq_scores.mean(dim=-1).mean().item()\n",
    "        assert np.abs(_avg_loss - batch_score) <= 1e-4, f\"Loss does not match: (batch: {input_ids})), {_avg_loss} - {batch_score} > 1e-6\"\n",
    "\n",
    "        seq_scores.extend(batch_seq_scores.mean(dim=-1).cpu().detach().numpy().tolist())\n",
    "        seq_trans_scores.extend(batch_seq_scores.cpu().detach().numpy())\n",
    "        \n",
    "    return seqs, seq_scores, np.stack(seq_trans_scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f8efecb",
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import defaultdict\n",
    "from tqdm import tqdm\n",
    "\n",
    "marginals = defaultdict(list)\n",
    "\n",
    "for template in tqdm(all_templates):\n",
    "    # print(\"Processing template:\", template)\n",
    "    res = compute_marginal_probability_attribute(template, \"{person}\", batch_size=64)\n",
    "    \n",
    "    marginals[\"template\"].extend([template] * tokenizer.vocab_size)\n",
    "    marginals[\"seq\"].extend(res[0])\n",
    "    marginals[\"seq_scores_sum\"].extend(res[2].sum(axis=1))\n",
    "    marginals[\"seq_scores_amean\"].extend(res[1])\n",
    "    marginals[\"seq_trans_scores\"].extend(res[2])\n",
    "    \n",
    "df_marginals = pd.DataFrame(marginals)\n",
    "df_marginals[\"seq_scores_sum_prob\"] = df_marginals[\"seq_scores_sum\"].apply(np.exp)\n",
    "\n",
    "# Determine whether the template is original or not (present in the benchmark)\n",
    "df_marginals[\"is_original\"] = df_marginals[\"seq\"].isin(df[\"Sentence\"])\n",
    "\n",
    "# Determine whether it is a male template\n",
    "df_marginals[\"male_seqs\"] = df_marginals[\"seq\"].isin(male_templates)\n",
    "\n",
    "# Determine whether it is a female template\n",
    "df_marginals[\"female_seqs\"] = df_marginals[\"seq\"].isin(female_templates)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0c872c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_marginals.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f327b0c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_marginals.to_csv(f\"eec_only_templates_all_vocab-{model_name2filename}.csv.gzip\", compression=\"gzip\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f755956",
   "metadata": {},
   "source": [
    "## Analysis\n",
    "\n",
    "In this section, we compute the templates"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b73e976",
   "metadata": {},
   "source": [
    "To combine multiple probabilities together we will have to convert the log probability of individual sequences to probabilities, sum across the group of interest and then, if desired, convert back to log probabilities."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d017bdcd",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_marginals.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "06666176",
   "metadata": {},
   "outputs": [],
   "source": [
    "# X-axis: probability of the templates\n",
    "# y-axis: log ratio between p(male words in template | template) and p(female words in template | template)\n",
    "male_mask = df_marginals[\"male_seqs\"]\n",
    "male_prob = df_marginals[male_mask].groupby(\"template\").sum().sort_index()[\"seq_scores_sum_prob\"]\n",
    "\n",
    "female_mask = df_marginals[\"female_seqs\"]\n",
    "female_prob = df_marginals[female_mask].groupby(\"template\").sum().sort_index()[\"seq_scores_sum_prob\"]\n",
    "\n",
    "all_prob = df_marginals.groupby(\"template\").sum()[\"seq_scores_sum_prob\"].sort_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1679246",
   "metadata": {},
   "outputs": [],
   "source": [
    "male_prob / female_prob.sort_values()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c89d56d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# We have both true and false because we're considering all the possible\n",
    "# completions for person, even the ones that did not occur in the original\n",
    "# dataset\n",
    "df_marginals[[\"template\", \"is_original\"]].drop_duplicates().values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "217b937e",
   "metadata": {},
   "outputs": [],
   "source": [
    "log_ratio = np.log(male_prob / female_prob)\n",
    "template_log_prob = np.log(all_prob)\n",
    "\n",
    "ax = sns.scatterplot(x=template_log_prob, y=log_ratio)\n",
    "plt.axhline(0, ls=\"--\")\n",
    "plt.xlabel(\"$log \\sum_{v \\in V} p_M(T_i, v \\in T_i)$\")\n",
    "plt.ylabel(\"log ratio $p(A|T_i)$/$p(B|T_i)$\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a78086e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "log_ratio[log_ratio > 2].sort_values()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48f45a8a",
   "metadata": {},
   "outputs": [],
   "source": [
    "log_ratio[log_ratio < -3].sort_values()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8743c92c",
   "metadata": {},
   "source": [
    "### What if we factor in the emotions? "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54e7bf3a",
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.DataFrame(template_log_prob)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "06b06b06",
   "metadata": {},
   "outputs": [],
   "source": [
    "d1, d2 = pd.DataFrame(log_ratio), pd.DataFrame(template_log_prob)\n",
    "temp = d1.join(d2, how=\"left\", lsuffix=\"_ratio\").reset_index()\n",
    "temp.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be12a10c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get emotion_word to emotion map\n",
    "word2emotion = {}\n",
    "for i, row in df[[\"Emotion\", \"Emotion word\"]].drop_duplicates().iterrows():\n",
    "    emotion = row[\"Emotion\"]\n",
    "    emotionword = row[\"Emotion word\"]\n",
    "    \n",
    "    word2emotion[emotion] = emotion\n",
    "    word2emotion[emotionword] = emotion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d795aba",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_emotion(template):\n",
    "    for em_w in emotion_words:\n",
    "        if em_w in template:\n",
    "            # return em_w\n",
    "            return word2emotion[em_w]\n",
    "    \n",
    "    for em in emotions:\n",
    "        if em in template:\n",
    "            return em\n",
    "    return \"No emotion\"\n",
    "\n",
    "temp[\"emotion\"] = temp[\"template\"].apply(extract_emotion)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f4822f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "ax = sns.scatterplot(data=temp, x=\"seq_scores_sum_prob\", y=\"seq_scores_sum_prob_ratio\", hue=\"emotion\")\n",
    "plt.axhline(0, ls=\"--\")\n",
    "plt.xlabel(\"$log \\sum_{v \\in V} p_M(T_i, v \\in T_i)$\")\n",
    "plt.ylabel(\"log ratio $p(A|T_i)$/$p(B|T_i)$\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1da4dbc",
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.displot(data=temp, x=\"seq_scores_sum_prob\", y=\"seq_scores_sum_prob_ratio\", hue=\"emotion\", kind=\"kde\", fill=True, alpha=0.5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e5a175f7",
   "metadata": {},
   "source": [
    "### Let us group the templates based on the different emotions and have a more granular view"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cbb0f748",
   "metadata": {},
   "outputs": [],
   "source": [
    "def aggregate_templates(template):\n",
    "    for em_w in emotion_words:\n",
    "        if em_w in template:\n",
    "            return template.replace(em_w, \"{emotion}\")\n",
    "    \n",
    "    for em in emotions:\n",
    "        if em in template:\n",
    "            return template.replace(em, \"{emotion}\")\n",
    "    \n",
    "    \n",
    "    return template"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1584d5e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_marginals[\"emotion\"] = df_marginals[\"template\"].apply(extract_emotion)\n",
    "df_marginals[\"original_template\"] = df_marginals[\"template\"].apply(aggregate_templates)\n",
    "df_marginals.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84415fcb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# X-axis: probability of the templates\n",
    "# y-axis: log ratio between p(male words in template | template) and p(female words in template | template)\n",
    "male_mask = df_marginals[\"male_seqs\"]\n",
    "male_prob = df_marginals[male_mask].groupby(\"original_template\").sum().sort_index()[\"seq_scores_sum_prob\"]\n",
    "\n",
    "female_mask = df_marginals[\"female_seqs\"]\n",
    "female_prob = df_marginals[female_mask].groupby(\"original_template\").sum().sort_index()[\"seq_scores_sum_prob\"]\n",
    "\n",
    "all_prob = df_marginals.groupby(\"original_template\").sum()[\"seq_scores_sum_prob\"].sort_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e107b927",
   "metadata": {},
   "outputs": [],
   "source": [
    "log_ratio = np.log(male_prob / female_prob)\n",
    "template_log_prob = np.log(all_prob)\n",
    "\n",
    "ax = sns.scatterplot(x=template_log_prob, y=log_ratio)\n",
    "plt.axhline(0, ls=\"--\")\n",
    "plt.xlabel(\"$log \\sum_{v \\in V} p_M(T_i, v \\in T_i)$\")\n",
    "plt.ylabel(\"log ratio $p(A|T_i)$/$p(B|T_i)$\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "972408e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "template_log_prob.sort_values(ascending=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0eaa83c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.exp(log_ratio).sort_values(ascending=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "500384ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_marginals[\"template\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66260327",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c83af5a1",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": true,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
